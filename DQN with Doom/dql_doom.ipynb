{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Instalando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (1.11.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (0.6.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (1.0.1)\n",
      "Requirement already satisfied: networkx>=1.8 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (0.20.0)\n",
      "Requirement already satisfied: pillow>=4.3.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from PyWavelets>=0.4.0->scikit-image) (1.15.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from dask[array]>=0.9.0->scikit-image) (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies. Vizdoom can be downloaded from https://github.com/mwydmuch/ViZDoom\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importando bibliotecas\n",
    "Para esse exemplo, teremos de importar todas os módulos ubíquos ao DRL, como:\n",
    "- TensorFlow\n",
    "- Numpy\n",
    "- Random\n",
    "\n",
    "E além disso, para nosso ambiente Doom, temos de importar o módulo responsável por modelar o ambiente Doom, chamado de Vizdoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from vizdoom import * # Doom environment module\n",
    "\n",
    "import random\n",
    "import time\n",
    "from skimage import transform  # For frame preprocessing\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings  # Ignores all warning messages from skimage during training\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Criando o ambiente Doom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating our Doom environment'''\n",
    "def create_env():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Loads a configuration file that handles all the options (size of frame, possible actions etc.)\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Loads a scenario. We're using basic scenario, but others can be used.\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # One-hot encoded list of possible actions.\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "\n",
    "'''Performing random actions to test the environment.'''\n",
    "def test_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(possible_actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print(\"\\tReward: \", reward)\n",
    "            time.sleep(0.02)\n",
    "        print(\"Result: \", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Pré processamento\n",
    "O pré processamento é utilizado para diminuírmos a dimensão do nosso input, que nesse caso são so frames do jogo. Nessa fase, iremos portanto pegar o frame input, converter as cores de RGB para grayscale, visto que as cores **não adicionam informação importante** para nossa rede, e então cortaremos parte do frame para que o teto não seja representado, visto que ele também não adiciona uma informações necessárias à nossa rede. Tudo isso será feito utilizando o módulo *skimage* importado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Frame preprocessing: takes a frame, grayscales and then downscales it.\"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Grayscaling is can be done by Vizdoom if configured properly in config file, but we're doing it 'manually'.\n",
    "    grayscaled_frame = np.mean(frame, 0)\n",
    "\n",
    "    # Cropping the frame to remove the roof, since it contains no relevant info.\n",
    "    cropped_frame = grayscaled_frame[30:-10, 30:-30]\n",
    "    \n",
    "    # Normalizing pixel color values.\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resizing the frame to a squared size (84x84).\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84])\n",
    "    return preprocessed_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamento de frames\n",
    "\n",
    "É necessário agrupar frames para **darmos à nossa rede uma noção de movimento**. Para isso, seguiremos os seguintes passos:\n",
    "\n",
    "- Pré processamos o frame atual\n",
    "- Empurramos esse frame a um *deque* que remove automaticamente o frame mais antigo\n",
    "- Depois disso, construímos o estado agrupado, que consiste em um estado representado pelo grupo de frames.\n",
    "\n",
    "O agrupamento funciona da seguinte forma:\n",
    "\n",
    "- Para o primeiro frame, nós usamos 4 frames.\n",
    "- A cada *timestep* adicionamos um novo frame ao *deque* e agrupamos eles para formarmos um novo frame agrupado.\n",
    "- Seguimos com esses passos até o final do episódio.\n",
    "- Ao final do episódio, repetimos o processo criando 4 novos frames, pois estamos em um novo episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # Number of frames we're stacking.\n",
    "\n",
    "# Initializing a stack of frames with empty (zero'd) frames.\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "# Stacked_frames = deque with stacked frames\n",
    "# state = current frame\n",
    "# is_new_episode = bool signaling if it's the start of an episode\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocessing the current frame\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_episode:\n",
    "        # If we're starting a new episode, the stack of frames is reinitialized with empty frames\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "        \n",
    "        # Then we fill the deque using the same frame, since we just started the episode and that's the only frame.\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Builds the stacked state from stacked frames. The frames have dimension (1, 84, 84) after preprocessing is done,\n",
    "        # so the stacked state has dimension (4, 84, 84), because we're using axis=2 to stack them.\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        # If we're not beginning a new episode, the current frame is stacked and the oldest is automatically removed.\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Building the stacked state from currently stacked frames.\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Definindo os hiperparâmetros\n",
    "\n",
    "Nessa parte nós definiremos os hiperparâmetros de nossa rede. Em um contexto real, os hiperparâmetros **não são definidos de uma vez logo quando construímos a rede, mas sim progressivamente durante o ciclo de desenvolvimento.**\n",
    "\n",
    "- Primeiro definiremos os hiperparâmetros da rede neural quando implementarmos o modelo.\n",
    "- Então, adicionaremos os hiperparâmetros de treinamento quando implementarmos o algoritmo de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>> NEURAL NETWORK HYPERPARAMETERS <<<<\n",
    "# As we've seen before, the state's dimension = 4 stacked frames, so we have (84, 84, 4)-sized inputs.\n",
    "state_size = [84, 84, 4]\n",
    "action_size = game.get_available_buttons_size() # We have 3 possible actions: left, right, shoot\n",
    "learning_rate = 0.0002 # The learning rate for our network. Tuning this value may yield better results.\n",
    "\n",
    "\n",
    "# >>>> TRAINING HYPERPARAMETERS <<<<\n",
    "total_episodes = 500 # Total #episodes for training\n",
    "max_steps = 100 # Maximum possible steps in an episode, considering we don't reach a terminal state.\n",
    "batch_size = 64\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy for action picking.\n",
    "explore_start = 1.0 # Exploration probability at the start of an episode.\n",
    "explore_stop = 0.01 # Exploration probability at the end of an episode.\n",
    "decay_rate = 0.0001 # Exponential decay rate for the exploration probability.\n",
    "\n",
    "# Q-Learning parameters\n",
    "gamma = 0.95 # Discount rate. Future rewards are multiplied by this value, so high values means future rewards are important.\n",
    "\n",
    "\n",
    "# >>>> MEMORY HYPERPARAMETERS <<<<\n",
    "pretrain_length = batch_size # Number of experience tuples stored in memory when it's first initialized.\n",
    "memory_size = 1000000 # Maximum number of experience tuples the memory can keep.\n",
    "\n",
    "\n",
    "# If training is set to false, we'll just see the trained agent, he'll try to follow the optimal policy so far.\n",
    "training = True\n",
    "# Set to True if you want to see the episode to be rendered, False otherwise.\n",
    "episode_render = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Criando o modelo de nossa Rede Neural com Deep Q-Learning\n",
    "\n",
    "Nosso modelo com Deep Q-Learning pode ser descrito da seguinte forma:\n",
    "\n",
    "- Primeiro nós agrupamos 4 frames como input, processo já descrito anteriormente;\n",
    "- Depois esse input passa por 3 camadas *convolucionais*;\n",
    "- É \"achatado\" (flattened);\n",
    "- Passa por mais duas camadas *densas*;\n",
    "- E por fim, o modelo nos dá como saída os **Q-Values** correspondentes a cada tipo de ação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # 'state_size' is the size of each state tuple, in our case, (4, 84, 84).\n",
    "            # So writing [None, *state_size] is the same as writing [None, 4, 84, 84].\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # As described in the article, targetQ = R(s, a) + Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            First convolutional layer:\n",
    "            CNN -> Batch normalization -> ELU activation\n",
    "            \"\"\"\n",
    "            \n",
    "            # The input of our network is a state, so the input size is (4, 84, 84).\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8, 8],\n",
    "                                          strides=[4, 4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                 training=True,\n",
    "                                                                 epsilon=1e-5,\n",
    "                                                                 name=\"batch_norm1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            \n",
    "            # Output size is (20, 20, 32).\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convolutional layer:\n",
    "            CNN -> Batch normalization -> ELU activation\n",
    "            \"\"\"\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[2, 2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "            \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                 training=True,\n",
    "                                                                 epsilon=1e-5,\n",
    "                                                                 name=\"batch_norm2\")\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            \n",
    "            # Output size is (9, 9, 64).\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convolutional layer:\n",
    "            CNN -> Batch normalization -> ELU activation\n",
    "            \"\"\"\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=128,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[2, 2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "            \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                 training=True,\n",
    "                                                                 epsilon=1e-5,\n",
    "                                                                 name=\"batch_norm3\")\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            \n",
    "            # Output size is (3, 3, 128)\n",
    "            \n",
    "            \"\"\"\n",
    "            Now we stack one flatten layer and two dense layers, the last one being the output layer.\n",
    "            \"\"\"\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            # Output size is 3 * 3 * 128 = 1152.\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                      name=\"fc1\")\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs=self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          units=3,\n",
    "                                          activation=None)\n",
    "            \n",
    "            # Those are the Q-Values predicted by our network.\n",
    "            # tf.reduce_sum returns a reduced version of the input tensor along specified axis.\n",
    "            # The 0 axis in tensorflow is rows, and the 1 axis is columns, so reduce_sum over axis 1 yields\n",
    "            # a tensor with one value for each row, summing over all of the row's values.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is the difference between the predicted Q-Values and Q_Target.\n",
    "            # tf.reduce_mean works like tf.reduce_sum, but instead of summing, it gets the mean.\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the default graph. This step is necessary for tensorflow to work properly.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiating the DQNetwork.\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Experience Replay\n",
    "\n",
    "A implementação do Experience Replay consiste em um *buffer* com **tuplas de experiências pelas quais o agente passou**, esse *buffer* permitirá ao agente a seleção de *minibatches* de experiências passadas para que elas sejam usadas no treinamento, permitindo que o agente **não aprenda apenas as experiências atuais e de forma sequencial, mas também experiências passadas**.\n",
    "\n",
    "Esse *buffer* é representado em nosso programa por um *deque*, que é uma fila de dois lados que **remove o seu elemento mais antigo quando um novo é adicionado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    # Starts the buffer's deque with limited size.\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    # Adds an element to the buffer.\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    # Samples a random batch from the buffer.\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Rendering the Doom environment\n",
    "game.new_episode()\n",
    "\n",
    "# Initializes the memory buffer by taking random actions and storing the experience\n",
    "# tuples (state, action, reward, new_state) in it.\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    # Taking a random action.\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Getting the reward from the chosen action.\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Checking if the episode is finished (reached a terminal state).\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Adding an experience tuple to the memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Getting the next state.\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Adding an experience tuple to the memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Configurando o TensorBoard\n",
    "\n",
    "O *TensorBoard* é uma ferramenta de análise do *TensorFlow*, assistir https://www.youtube.com/watch?v=eBbEDRsCmv4 .\n",
    "\n",
    "Para executar o *TensorBoard*, devemos utilizar o comando *tensorboard --logdir=/tensorboard/dqn/1* no CMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the TensorBoard writer.\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "# Writing loss values to TensorBoard\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Treinando nosso agente\n",
    "\n",
    "O algoritmo de treinamento que utilizaremos segue os seguintes passos:\n",
    "\n",
    "- **Inicializa** os *pesos*;\n",
    "- **Inicializa** o *ambiente*;\n",
    "- **Inicializa** a *taxa de decaimento* de *epsilon* (epsilon é usado pela seleção de ação epsilon greedy);\n",
    "\n",
    "\n",
    "- **Para cada** *episódio* **em** *max_episódios*:\n",
    "    - **Cria** um novo episódio;\n",
    "    - **Setta** *step* para 0;\n",
    "    - **Observa** o primeiro estado S0;\n",
    "    \n",
    "    - **Enquanto** *step* < *max_step*:\n",
    "        - **Incrementa** a *taxa de decaimento*;\n",
    "        - Com *prob* = *epsilon*, **seleciona** uma ação aleatória a, ou a = *argmaxQ(s, a)*;\n",
    "        - **Simula** com a ação *a* e **observa** a recompensa R,t+1 e novo estado s,t+1;\n",
    "        - **Armazena** a transição $;\n",
    "        - **Amostra** um *mini-batch* aleatório de *D*: $$\n",
    "        - **Setta** *Q,hat* = *R* **se** o episódio termina em +1, caso contrário, **setta** Q,hat = r + decay . max_a . Q(s', a')\n",
    "        - **Executa** um *step* do gradiente descendente com *perda* (Q,hat - Q(s, a)) ^ 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function is responsible for handling epsilon greedy action selection and epsilon decay.\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    # >>>> EPSILON GREEDY STRATEGY <<<<\n",
    "    # Choosing action a from state s using epsilon greedy\n",
    "    \n",
    "    # First, we pick a random number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    # np.exp calculates the exponential of input parameter.\n",
    "    # Calculating the explore probability. Exploring is the same as picking a random action.\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        # We take a random action\n",
    "        action = random.choice(actions)\n",
    "        \n",
    "    else:\n",
    "        # We get the current best action from our Q-Network. This is exploitation, the opposite of exploration.\n",
    "        \n",
    "        # Estimating the Q-Values for state.\n",
    "        Qs = sess.run(DQNetwork.output,\n",
    "                      feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Getting the biggest Q-Value from our estimated Q-Values. This will yield us the best action.\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 93.0 Training loss: 162.3175 Explore prob: 0.9992\n",
      "Saved model!\n",
      "Episode: 1 Total reward: 94.0 Training loss: 281.6517 Explore prob: 0.9985\n",
      "Episode: 2 Total reward: 94.0 Training loss: 299.9031 Explore prob: 0.9978\n",
      "Episode: 3 Total reward: 95.0 Training loss: 115.8068 Explore prob: 0.9972\n",
      "Episode: 5 Total reward: 90.0 Training loss: 85.9537 Explore prob: 0.9863\n",
      "Saved model!\n",
      "Episode: 6 Total reward: -18.0 Training loss: 13.0123 Explore prob: 0.9772\n",
      "Episode: 7 Total reward: 75.0 Training loss: 16.7627 Explore prob: 0.9752\n",
      "Episode: 8 Total reward: 94.0 Training loss: 28.2834 Explore prob: 0.9745\n",
      "Saved model!\n",
      "Episode: 13 Total reward: 95.0 Training loss: 5.0644 Explore prob: 0.9361\n",
      "Episode: 15 Total reward: 94.0 Training loss: 5.0752 Explore prob: 0.9263\n",
      "Saved model!\n",
      "Saved model!\n",
      "Episode: 22 Total reward: 95.0 Training loss: 6.6279 Explore prob: 0.8724\n",
      "Episode: 24 Total reward: 93.0 Training loss: 116.1769 Explore prob: 0.8631\n",
      "Episode: 25 Total reward: 91.0 Training loss: 3.1956 Explore prob: 0.8623\n",
      "Saved model!\n",
      "Episode: 28 Total reward: 95.0 Training loss: 4.7953 Explore prob: 0.8449\n",
      "Episode: 29 Total reward: 89.0 Training loss: 5.0676 Explore prob: 0.8439\n",
      "Episode: 30 Total reward: 92.0 Training loss: 1.6261 Explore prob: 0.8431\n",
      "Saved model!\n",
      "Episode: 31 Total reward: 27.0 Training loss: 126.9682 Explore prob: 0.8378\n",
      "Episode: 33 Total reward: 95.0 Training loss: 8.3147 Explore prob: 0.8291\n",
      "Episode: 34 Total reward: 90.0 Training loss: 3.2117 Explore prob: 0.8282\n",
      "Saved model!\n",
      "Episode: 36 Total reward: 55.0 Training loss: 2.9985 Explore prob: 0.8167\n",
      "Episode: 37 Total reward: 95.0 Training loss: 4.5423 Explore prob: 0.8163\n",
      "Saved model!\n",
      "Episode: 41 Total reward: 94.0 Training loss: 1.8729 Explore prob: 0.7919\n",
      "Episode: 42 Total reward: 21.0 Training loss: 7.9107 Explore prob: 0.7868\n",
      "Episode: 43 Total reward: 91.0 Training loss: 3.8394 Explore prob: 0.7860\n",
      "Episode: 44 Total reward: 16.0 Training loss: 2.8444 Explore prob: 0.7806\n",
      "Episode: 45 Total reward: 85.0 Training loss: 4.6548 Explore prob: 0.7794\n",
      "Saved model!\n",
      "Episode: 46 Total reward: 92.0 Training loss: 6.6763 Explore prob: 0.7787\n",
      "Episode: 49 Total reward: 94.0 Training loss: 2.0211 Explore prob: 0.7630\n",
      "Saved model!\n",
      "Episode: 55 Total reward: 95.0 Training loss: 4.8306 Explore prob: 0.7258\n",
      "Saved model!\n",
      "Episode: 56 Total reward: 94.0 Training loss: 10.6719 Explore prob: 0.7253\n",
      "Episode: 57 Total reward: 81.0 Training loss: 3.9507 Explore prob: 0.7239\n",
      "Episode: 58 Total reward: 94.0 Training loss: 4.2682 Explore prob: 0.7234\n",
      "Episode: 59 Total reward: 40.0 Training loss: 3.5250 Explore prob: 0.7197\n",
      "Episode: 60 Total reward: 18.0 Training loss: 14.5700 Explore prob: 0.7149\n",
      "Saved model!\n",
      "Episode: 61 Total reward: 64.0 Training loss: 11.2241 Explore prob: 0.7127\n",
      "Episode: 62 Total reward: 69.0 Training loss: 23.0960 Explore prob: 0.7108\n",
      "Episode: 64 Total reward: 95.0 Training loss: 6.3608 Explore prob: 0.7034\n",
      "Episode: 65 Total reward: 62.0 Training loss: 7.8258 Explore prob: 0.7010\n",
      "Saved model!\n",
      "Episode: 66 Total reward: 36.0 Training loss: 6.4631 Explore prob: 0.6976\n",
      "Episode: 67 Total reward: -5.0 Training loss: 3.8062 Explore prob: 0.6917\n",
      "Episode: 68 Total reward: 94.0 Training loss: 6.3386 Explore prob: 0.6912\n",
      "Episode: 69 Total reward: 75.0 Training loss: 8.4638 Explore prob: 0.6898\n",
      "Episode: 70 Total reward: 95.0 Training loss: 7.2616 Explore prob: 0.6894\n",
      "Saved model!\n",
      "Episode: 71 Total reward: 70.0 Training loss: 5.6186 Explore prob: 0.6876\n",
      "Episode: 72 Total reward: 95.0 Training loss: 5.1636 Explore prob: 0.6872\n",
      "Episode: 73 Total reward: 95.0 Training loss: 6.2543 Explore prob: 0.6868\n",
      "Saved model!\n",
      "Episode: 76 Total reward: 94.0 Training loss: 10.9294 Explore prob: 0.6730\n",
      "Episode: 78 Total reward: 94.0 Training loss: 5.9909 Explore prob: 0.6659\n",
      "Episode: 79 Total reward: 95.0 Training loss: 10.9912 Explore prob: 0.6655\n",
      "Episode: 80 Total reward: 95.0 Training loss: 5.8518 Explore prob: 0.6651\n",
      "Saved model!\n",
      "Episode: 81 Total reward: 90.0 Training loss: 6.1436 Explore prob: 0.6644\n",
      "Episode: 82 Total reward: 95.0 Training loss: 10.8146 Explore prob: 0.6640\n",
      "Episode: 83 Total reward: -23.0 Training loss: 6.3767 Explore prob: 0.6576\n",
      "Episode: 84 Total reward: 93.0 Training loss: 7.3667 Explore prob: 0.6570\n",
      "Episode: 85 Total reward: 95.0 Training loss: 5.5269 Explore prob: 0.6566\n",
      "Saved model!\n",
      "Episode: 87 Total reward: 95.0 Training loss: 103.3088 Explore prob: 0.6498\n",
      "Episode: 89 Total reward: 43.0 Training loss: 4.6234 Explore prob: 0.6404\n",
      "Episode: 90 Total reward: 92.0 Training loss: 3.4956 Explore prob: 0.6399\n",
      "Saved model!\n",
      "Episode: 91 Total reward: 41.0 Training loss: 4.9854 Explore prob: 0.6367\n",
      "Episode: 92 Total reward: 92.0 Training loss: 2.8718 Explore prob: 0.6362\n",
      "Episode: 93 Total reward: 94.0 Training loss: 6.6030 Explore prob: 0.6357\n",
      "Episode: 94 Total reward: 17.0 Training loss: 5.2851 Explore prob: 0.6314\n",
      "Episode: 95 Total reward: 95.0 Training loss: 9.1463 Explore prob: 0.6310\n",
      "Saved model!\n",
      "Episode: 96 Total reward: 43.0 Training loss: 9.3383 Explore prob: 0.6281\n",
      "Episode: 97 Total reward: 68.0 Training loss: 8.1925 Explore prob: 0.6263\n",
      "Episode: 98 Total reward: 50.0 Training loss: 4.8209 Explore prob: 0.6238\n",
      "Saved model!\n",
      "Episode: 101 Total reward: 88.0 Training loss: 7.5782 Explore prob: 0.6109\n",
      "Episode: 102 Total reward: 95.0 Training loss: 8.2058 Explore prob: 0.6105\n",
      "Episode: 103 Total reward: 94.0 Training loss: 5.1313 Explore prob: 0.6101\n",
      "Episode: 104 Total reward: 90.0 Training loss: 6.7583 Explore prob: 0.6094\n",
      "Saved model!\n",
      "Episode: 106 Total reward: 93.0 Training loss: 11.7136 Explore prob: 0.6030\n",
      "Episode: 107 Total reward: 95.0 Training loss: 4.9138 Explore prob: 0.6027\n",
      "Episode: 108 Total reward: 94.0 Training loss: 6.6242 Explore prob: 0.6022\n",
      "Episode: 109 Total reward: 94.0 Training loss: 5.0365 Explore prob: 0.6018\n",
      "Saved model!\n",
      "Episode: 111 Total reward: 94.0 Training loss: 8.2214 Explore prob: 0.5955\n",
      "Episode: 112 Total reward: -10.0 Training loss: 6.9498 Explore prob: 0.5905\n",
      "Episode: 113 Total reward: 49.0 Training loss: 11.2243 Explore prob: 0.5881\n",
      "Episode: 114 Total reward: 56.0 Training loss: 7.5907 Explore prob: 0.5858\n",
      "Episode: 115 Total reward: 95.0 Training loss: 17.6167 Explore prob: 0.5854\n",
      "Saved model!\n",
      "Episode: 116 Total reward: 95.0 Training loss: 13.2041 Explore prob: 0.5851\n",
      "Episode: 117 Total reward: 95.0 Training loss: 10.8289 Explore prob: 0.5847\n",
      "Episode: 118 Total reward: 75.0 Training loss: 6.7865 Explore prob: 0.5835\n",
      "Episode: 120 Total reward: 95.0 Training loss: 12.3887 Explore prob: 0.5775\n",
      "Saved model!\n",
      "Episode: 121 Total reward: 95.0 Training loss: 8.4318 Explore prob: 0.5771\n",
      "Episode: 122 Total reward: 95.0 Training loss: 5.0928 Explore prob: 0.5768\n",
      "Episode: 123 Total reward: 50.0 Training loss: 3.3466 Explore prob: 0.5745\n",
      "Episode: 124 Total reward: 71.0 Training loss: 3.5511 Explore prob: 0.5731\n",
      "Episode: 125 Total reward: 91.0 Training loss: 8.1168 Explore prob: 0.5725\n",
      "Saved model!\n",
      "Episode: 126 Total reward: 95.0 Training loss: 8.6793 Explore prob: 0.5722\n",
      "Episode: 128 Total reward: 94.0 Training loss: 6.0084 Explore prob: 0.5662\n",
      "Episode: 129 Total reward: 63.0 Training loss: 6.5762 Explore prob: 0.5644\n",
      "Episode: 130 Total reward: 95.0 Training loss: 3.6224 Explore prob: 0.5640\n",
      "Saved model!\n",
      "Episode: 131 Total reward: 28.0 Training loss: 10.0490 Explore prob: 0.5605\n",
      "Episode: 132 Total reward: 95.0 Training loss: 5.4570 Explore prob: 0.5602\n",
      "Episode: 133 Total reward: 94.0 Training loss: 5.7728 Explore prob: 0.5598\n",
      "Saved model!\n",
      "Episode: 136 Total reward: 93.0 Training loss: 1.8604 Explore prob: 0.5485\n",
      "Episode: 137 Total reward: 95.0 Training loss: 5.8683 Explore prob: 0.5482\n",
      "Episode: 138 Total reward: 42.0 Training loss: 4.0856 Explore prob: 0.5456\n",
      "Episode: 139 Total reward: 95.0 Training loss: 10.3181 Explore prob: 0.5452\n",
      "Episode: 140 Total reward: 73.0 Training loss: 7.5395 Explore prob: 0.5440\n",
      "Saved model!\n",
      "Episode: 141 Total reward: 92.0 Training loss: 4.7531 Explore prob: 0.5435\n",
      "Episode: 142 Total reward: 95.0 Training loss: 9.7959 Explore prob: 0.5432\n",
      "Episode: 143 Total reward: 95.0 Training loss: 3.7421 Explore prob: 0.5429\n",
      "Episode: 144 Total reward: 92.0 Training loss: 3.9611 Explore prob: 0.5424\n",
      "Episode: 145 Total reward: 89.0 Training loss: 4.7400 Explore prob: 0.5418\n",
      "Saved model!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 146 Total reward: 20.0 Training loss: 4.2128 Explore prob: 0.5383\n",
      "Episode: 147 Total reward: 95.0 Training loss: 6.3267 Explore prob: 0.5380\n",
      "Episode: 148 Total reward: 94.0 Training loss: 7.3650 Explore prob: 0.5376\n",
      "Episode: 149 Total reward: 95.0 Training loss: 11.3750 Explore prob: 0.5373\n",
      "Episode: 150 Total reward: 94.0 Training loss: 4.0003 Explore prob: 0.5369\n",
      "Saved model!\n",
      "Episode: 151 Total reward: 95.0 Training loss: 11.4416 Explore prob: 0.5366\n",
      "Episode: 152 Total reward: 95.0 Training loss: 7.9479 Explore prob: 0.5363\n",
      "Episode: 154 Total reward: 73.0 Training loss: 6.5760 Explore prob: 0.5298\n",
      "Episode: 155 Total reward: 94.0 Training loss: 7.1555 Explore prob: 0.5295\n",
      "Saved model!\n",
      "Episode: 157 Total reward: 95.0 Training loss: 8.7573 Explore prob: 0.5240\n",
      "Episode: 158 Total reward: 95.0 Training loss: 6.1254 Explore prob: 0.5237\n",
      "Episode: 159 Total reward: 76.0 Training loss: 9.6282 Explore prob: 0.5227\n",
      "Episode: 160 Total reward: 8.0 Training loss: 17.2523 Explore prob: 0.5187\n",
      "Saved model!\n",
      "Episode: 162 Total reward: -3.0 Training loss: 4.0244 Explore prob: 0.5094\n",
      "Episode: 163 Total reward: 93.0 Training loss: 5.2247 Explore prob: 0.5090\n",
      "Episode: 164 Total reward: 94.0 Training loss: 5.9162 Explore prob: 0.5087\n",
      "Episode: 165 Total reward: 95.0 Training loss: 6.5705 Explore prob: 0.5084\n",
      "Saved model!\n",
      "Episode: 167 Total reward: 46.0 Training loss: 8.2518 Explore prob: 0.5012\n",
      "Episode: 168 Total reward: 94.0 Training loss: 18.7851 Explore prob: 0.5008\n",
      "Episode: 169 Total reward: 95.0 Training loss: 5.5656 Explore prob: 0.5005\n",
      "Episode: 170 Total reward: 69.0 Training loss: 8.5436 Explore prob: 0.4992\n",
      "Saved model!\n",
      "Episode: 171 Total reward: 46.0 Training loss: 7.7517 Explore prob: 0.4970\n",
      "Episode: 172 Total reward: 64.0 Training loss: 6.8834 Explore prob: 0.4955\n",
      "Episode: 173 Total reward: 95.0 Training loss: 8.1490 Explore prob: 0.4952\n",
      "Episode: 175 Total reward: 95.0 Training loss: 2.1385 Explore prob: 0.4901\n",
      "Saved model!\n",
      "Episode: 176 Total reward: 95.0 Training loss: 20.9082 Explore prob: 0.4898\n",
      "Episode: 177 Total reward: 76.0 Training loss: 7.7908 Explore prob: 0.4888\n",
      "Episode: 178 Total reward: 95.0 Training loss: 6.5800 Explore prob: 0.4885\n",
      "Episode: 179 Total reward: 95.0 Training loss: 5.0908 Explore prob: 0.4882\n",
      "Episode: 180 Total reward: 69.0 Training loss: 5.6777 Explore prob: 0.4869\n",
      "Saved model!\n",
      "Episode: 181 Total reward: 74.0 Training loss: 6.1398 Explore prob: 0.4859\n",
      "Episode: 182 Total reward: 95.0 Training loss: 5.5727 Explore prob: 0.4856\n",
      "Episode: 183 Total reward: 75.0 Training loss: 6.9902 Explore prob: 0.4846\n",
      "Episode: 184 Total reward: 95.0 Training loss: 10.8726 Explore prob: 0.4843\n",
      "Episode: 185 Total reward: 44.0 Training loss: 6.5095 Explore prob: 0.4821\n",
      "Saved model!\n",
      "Episode: 187 Total reward: 22.0 Training loss: 6.4354 Explore prob: 0.4744\n",
      "Episode: 188 Total reward: 95.0 Training loss: 10.7194 Explore prob: 0.4741\n",
      "Episode: 190 Total reward: 95.0 Training loss: 3.5624 Explore prob: 0.4693\n",
      "Saved model!\n",
      "Episode: 191 Total reward: 95.0 Training loss: 6.5415 Explore prob: 0.4690\n",
      "Episode: 192 Total reward: 95.0 Training loss: 12.4216 Explore prob: 0.4687\n",
      "Episode: 193 Total reward: 65.0 Training loss: 4.2590 Explore prob: 0.4673\n",
      "Episode: 194 Total reward: 72.0 Training loss: 7.3953 Explore prob: 0.4662\n",
      "Episode: 195 Total reward: 89.0 Training loss: 8.9583 Explore prob: 0.4656\n",
      "Saved model!\n",
      "Episode: 196 Total reward: 95.0 Training loss: 9.4314 Explore prob: 0.4654\n",
      "Episode: 197 Total reward: 95.0 Training loss: 5.8014 Explore prob: 0.4651\n",
      "Episode: 198 Total reward: 95.0 Training loss: 4.9627 Explore prob: 0.4648\n"
     ]
    }
   ],
   "source": [
    "# As the name implies, saver is responsible for saving our model.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initializing our Variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initializing the decay rate used to decrement from epsilon.\n",
    "        decay_step = 0\n",
    "        \n",
    "        # Initializing the game\n",
    "        game.init()\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # >>>> SIMULATION PART <<<<\n",
    "            ### In this part of the algorithm, we're only simulating the environment and recording rewards and memory states.\n",
    "            \n",
    "            # Reinitializing the episode's variables.\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            game.new_episode() # Starting a new episode\n",
    "            \n",
    "            state = game.get_state().screen_buffer  # Getting the first frame\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True) # Getting a stack of frames to use as input\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                \n",
    "                # Choosing an action using epsilon greedy strategy.\n",
    "                action, explore_probability = predict_action(explore_start,\n",
    "                                                             explore_stop,\n",
    "                                                             decay_rate,\n",
    "                                                             decay_step,\n",
    "                                                             state,\n",
    "                                                             possible_actions)\n",
    "                \n",
    "                reward = game.make_action(action)  # Executing the chosen action.\n",
    "                done = game.is_episode_finished()\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # If reached terminal state\n",
    "                if done:\n",
    "                    next_state = np.zeros((3, 84, 84), dtype=np.int)  # next_state is set to empty, because there is no next step.\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print(\"Episode: {}\".format(episode),\n",
    "                            \"Total reward: {}\".format(total_reward),\n",
    "                            \"Training loss: {:.4f}\".format(loss),\n",
    "                            \"Explore prob: {:.4f}\".format(explore_probability))\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                    \n",
    "                # >>>> LEARNING PART <<<<\n",
    "                \n",
    "                # Getting minibatches of experience tuples [(state, action, reward, next_state, done)] from the memory.\n",
    "                memory_batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([xp_tuple[0] for xp_tuple in memory_batch], ndmin=3)\n",
    "                actions_mb = np.array([xp_tuple[1] for xp_tuple in memory_batch])\n",
    "                rewards_mb = np.array([xp_tuple[2] for xp_tuple in memory_batch])\n",
    "                next_states_mb = np.array([xp_tuple[3] for xp_tuple in memory_batch], ndmin=3)\n",
    "                dones_mb = np.array([xp_tuple[4] for xp_tuple in memory_batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                # Getting Q-Values for the next state.\n",
    "                Qs_next_state = sess.run(DQNetwork.output,\n",
    "                                         feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #  Sets Q_target = r if the episode ends after s+1, otherwise, we set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(memory_batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    # The state is terminal, so we set Q_target = reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                \n",
    "                targets_mb = np.array([targetQ for targetQ in target_Qs_batch])\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                   feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                              DQNetwork.target_Q: targets_mb,\n",
    "                                              DQNetwork.actions_: actions_mb})\n",
    "                \n",
    "                # Writing TensorFlow summaries.\n",
    "                summary = sess.run(write_op,\n",
    "                                   feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                              DQNetwork.target_Q: targets_mb,\n",
    "                                              DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()    \n",
    "            if episode % 5 == 0:\n",
    "                #save_path = saver.save(sess, \"models/model.ckpt\")\n",
    "                print(\"Saved model!\")\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Visualizando o nosso agente.\n",
    "\n",
    "Agora que treinamos nosso agente, nós podemos testá-lo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Score:  72.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  68.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  91.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  38.0\n",
      "Score:  95.0\n",
      "Score:  76.0\n",
      "Score:  95.0\n",
      "Score:  -385.0\n",
      "Score:  -375.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  64.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -375.0\n",
      "Score:  95.0\n",
      "Score:  66.0\n",
      "Score:  95.0\n",
      "Score:  -380.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -390.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -9.0\n",
      "Score:  72.0\n",
      "Score:  50.0\n",
      "Score:  95.0\n",
      "Score:  -390.0\n",
      "Score:  95.0\n",
      "Score:  -385.0\n",
      "Score:  95.0\n",
      "Score:  -410.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  23.0\n",
      "Score:  95.0\n",
      "Score:  -380.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -365.0\n",
      "Score:  95.0\n",
      "Score:  -365.0\n",
      "Score:  95.0\n",
      "Score:  -5.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -123.0\n",
      "Score:  22.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -335.0\n",
      "Score:  91.0\n",
      "Score:  69.0\n",
      "Score:  -183.0\n",
      "Score:  -138.0\n",
      "Score:  95.0\n",
      "Score:  -405.0\n",
      "Score:  -400.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -142.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  38.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -243.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  -400.0\n",
      "Score:  -385.0\n",
      "Score:  95.0\n",
      "Score:  -380.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "TOTAL SCORE:  -6.46\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    game, possible_actions = create_env()\n",
    "    total_score = 0\n",
    "    \n",
    "    saver.restore(sess, \"models/model.ckpt\") # Loading our saved trained model.\n",
    "    \n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        first_frame = True\n",
    "        \n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            if first_frame:\n",
    "                frame = game.get_state().screen_buffer\n",
    "                state, stacked_frames = stack_frames(stacked_frames, frame, True)\n",
    "                first_frame = False\n",
    "            else:\n",
    "                frame = game.get_state().screen_buffer\n",
    "                state, stacked_frames = stack_frames(stacked_frames, frame, False)\n",
    "            \n",
    "            # Taking the biggest Q-Value from our network, which maps to the best action.\n",
    "            Qs = sess.run(DQNetwork.output,\n",
    "                          feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            action = np.argmax(Qs)\n",
    "            action = possible_actions[int(action)]\n",
    "            \n",
    "            game.make_action(action) # Executing the action picked.\n",
    "            \n",
    "            score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "        total_score += score\n",
    "    print(\"TOTAL SCORE: \", total_score/100.0)\n",
    "    game.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
