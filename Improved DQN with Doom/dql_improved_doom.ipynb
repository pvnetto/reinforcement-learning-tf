{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Instalando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from scikit-image) (1.11.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from scikit-image) (0.5.6)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from scikit-image) (1.0.1)\n",
      "Requirement already satisfied: networkx>=1.8 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: pillow>=4.3.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from scikit-image) (5.2.0)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from scikit-image) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from PyWavelets>=0.4.0->scikit-image) (1.14.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from dask[array]>=0.9.0->scikit-image) (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (4.26.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies. Vizdoom can be downloaded from https://github.com/mwydmuch/ViZDoom\n",
    "!pip install scikit-image\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importando bibliotecas\n",
    "Para esse exemplo, teremos de importar todas os módulos ubíquos ao DRL, como:\n",
    "- TensorFlow\n",
    "- Numpy\n",
    "- Random\n",
    "\n",
    "E além disso, para nosso ambiente Doom, temos de importar o módulo responsável por modelar o ambiente Doom, chamado de Vizdoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from vizdoom import * # Doom environment module\n",
    "\n",
    "import random\n",
    "import time\n",
    "from tqdm import *\n",
    "from skimage import transform  # For frame preprocessing\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings  # Ignores all warning messages from skimage during training\n",
    "warnings.filterwarnings(\"ignore\")# 2) Criando o ambiente Doom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Criando o ambiente Doom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating our Doom environment'''\n",
    "config_filename = \"deadly_corridor.cfg\"\n",
    "scenario_filename = \"deadly_corridor.wad\"\n",
    "def create_env():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Loads a configuration file that handles all the options (size of frame, possible actions etc.)\n",
    "    game.load_config(config_filename)\n",
    "    \n",
    "    # Loads a scenario. We're using basic scenario, but others can be used.\n",
    "    game.set_doom_scenario_path(scenario_filename)\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # One-hot encoded list of possible actions. We can:\n",
    "    # turn left, turn right, move left, move right, and shoot.\n",
    "    possible_actions = np.identity(7, dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "\n",
    "'''Performing random actions to test the environment.'''\n",
    "def test_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config(config_filename)\n",
    "    game.set_doom_scenario_path(scenario_filename)\n",
    "    game.init()\n",
    "    \n",
    "    possible_actions = np.identity(7, dtype=int).tolist()\n",
    "    \n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(possible_actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print(\"\\tReward: \", reward)\n",
    "            time.sleep(0.02)\n",
    "        print(\"Result: \", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Pré processamento\n",
    "O pré processamento é utilizado para diminuírmos a dimensão do nosso input, que nesse caso são so frames do jogo. Nessa fase, iremos portanto pegar o frame input, converter as cores de RGB para grayscale, visto que as cores **não adicionam informação importante** para nossa rede, e então cortaremos parte do frame para que o teto não seja representado, visto que ele também não adiciona uma informações necessárias à nossa rede. Tudo isso será feito utilizando o módulo *skimage* importado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Frame preprocessing: takes a frame, grayscales and then downscales it.\"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Grayscaling is can be done by Vizdoom if configured properly in config file, but we're doing it 'manually'.\n",
    "    grayscaled_frame = np.mean(frame, 0)\n",
    "\n",
    "    # Cropping the frame to remove the roof, since it contains no relevant info.\n",
    "    cropped_frame = grayscaled_frame[15:-5, 20:-20]\n",
    "    \n",
    "    # Normalizing pixel color values.\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resizing the frame to a squared size (84x84).\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [100, 120])\n",
    "    return preprocessed_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamento de frames\n",
    "\n",
    "É necessário agrupar frames para **darmos à nossa rede uma noção de movimento**. Para isso, seguiremos os seguintes passos:\n",
    "\n",
    "- Pré processamos o frame atual\n",
    "- Empurramos esse frame a um *deque* que remove automaticamente o frame mais antigo\n",
    "- Depois disso, construímos o estado agrupado, que consiste em um estado representado pelo grupo de frames.\n",
    "\n",
    "O agrupamento funciona da seguinte forma:\n",
    "\n",
    "- Para o primeiro frame, nós usamos 4 frames.\n",
    "- A cada *timestep* adicionamos um novo frame ao *deque* e agrupamos eles para formarmos um novo frame agrupado.\n",
    "- Seguimos com esses passos até o final do episódio.\n",
    "- Ao final do episódio, repetimos o processo criando 4 novos frames, pois estamos em um novo episódio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # Number of frames we're stacking.\n",
    "\n",
    "# Initializing a stack of frames with empty (zero'd) frames.\n",
    "stacked_frames = deque([np.zeros((100, 120), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "# Stacked_frames = deque with stacked frames\n",
    "# state = current frame\n",
    "# is_new_episode = bool signaling if it's the start of an episode\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocessing the current frame\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_episode:\n",
    "        # If we're starting a new episode, the stack of frames is reinitialized with empty frames\n",
    "        stacked_frames = deque([np.zeros((100, 120), dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "        \n",
    "        # Then we fill the deque using the same frame, since we just started the episode and that's the only frame.\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Builds the stacked state from stacked frames. The frames have dimension (1, 84, 84) after preprocessing is done,\n",
    "        # so the stacked state has dimension (4, 84, 84), because we're using axis=2 to stack them.\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        # If we're not beginning a new episode, the current frame is stacked and the oldest is automatically removed.\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Building the stacked state from currently stacked frames.\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Definindo os hiperparâmetros\n",
    "\n",
    "Nessa parte nós definiremos os hiperparâmetros de nossa rede. Em um contexto real, os hiperparâmetros **não são definidos de uma vez logo quando construímos a rede, mas sim progressivamente durante o ciclo de desenvolvimento.**\n",
    "\n",
    "- Primeiro definiremos os hiperparâmetros da rede neural quando implementarmos o modelo.\n",
    "- Então, adicionaremos os hiperparâmetros de treinamento quando implementarmos o algoritmo de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>> NEURAL NETWORK HYPERPARAMETERS <<<<\n",
    "# As we've seen before, the state's dimension = 4 stacked frames, so we have (84, 84, 4)-sized inputs.\n",
    "state_size = [100, 120, 4]\n",
    "action_size = game.get_available_buttons_size() # We have 7 possible actions\n",
    "learning_rate = 0.00025 # The learning rate for our network. Tuning this value may yield better results.\n",
    "\n",
    "\n",
    "# >>>> TRAINING HYPERPARAMETERS <<<<\n",
    "total_episodes = 5000 # Total #episodes for training\n",
    "max_steps = 5000 # Maximum possible steps in an episode, considering we don't reach a terminal state.\n",
    "batch_size = 64\n",
    "\n",
    "# >>>> FIXED Q TARGETS STRATEGY PARAMETERS <<<<\n",
    "max_tau = 10000 # The maximum number of steps it takes without updating Q-Target\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy for action picking.\n",
    "explore_start = 1.0 # Exploration probability at the start of an episode.\n",
    "explore_stop = 0.01 # Exploration probability at the end of an episode.\n",
    "decay_rate = 0.00005 # Exponential decay rate for the exploration probability.\n",
    "\n",
    "# Q-Learning parameters\n",
    "gamma = 0.95 # Discount rate. Future rewards are multiplied by this value, so high values means future rewards are important.\n",
    "\n",
    "\n",
    "# >>>> MEMORY HYPERPARAMETERS <<<<\n",
    "pretrain_length = 10000 # Number of experience tuples stored in memory when it's first initialized.\n",
    "memory_size = 10000 # Maximum number of experience tuples the memory can keep.\n",
    "\n",
    "\n",
    "# If training is set to false, we'll just see the trained agent, he'll try to follow the optimal policy so far.\n",
    "training = True\n",
    "# Set to True if you want to see the episode to be rendered, False otherwise.\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Criando nossa Dueling Double Deep Q-Learning Network (DDDQNN)\n",
    "\n",
    "Os **Q-Values** representam o quão bom é usar determinada ação *a* estando em um estado *s*. Ele pode ser decomposto como a soma de:\n",
    "\n",
    "- **V(s)**: O valor de estar no estado *s*.\n",
    "- **A(s, a)**: A vantagem de usar uma ação a dado um estado s.\n",
    "\n",
    "Podemos portanto usar uma **DDDQNN** para **desacoplar a estimativa dos valores de V(s) e A(s, a) usando duas camadas separadas, uma para cada valor.**\n",
    "\n",
    "A vantagem dessa abordagem é que podemos **usar V(s) para determinar o valor de um estado sem ter que aprender o valor de cada ação para esse estado**.  Isso é bastante útil, pois **há estados em que a ação que tomamos não influencia no resultado**, e se estivéssemos usando uma **DQN** normal, teríamos de calcular o valor de cada ação para esse estado, coisa que podemos evitar com a **DDDQNN**.\n",
    "\n",
    "Nosso modelo de DDDQNN tem a seguinte forma:\n",
    "\n",
    "- Input: 4 frames agrupados;\n",
    "- 3 camadas convolucionais;\n",
    "- Uma camada flatten;\n",
    "- Duas camadas densas:\n",
    "    - Uma para calcular V(s).\n",
    "    - Outra para calcular os A(s, a) para cada ação.\n",
    "- Uma camada de agregação que junta esses valores;\n",
    "- Output: Q-Values para cada ação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name):\n",
    "        # Constructing DDQNet's parameters.\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        # tf.variable_scope is used to know which network we're using (DQN or target net). It will be useful\n",
    "        # to update our w- parameters (the strategy of fixed Q-Targets).\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            # Creating the placeholders, which are initialized along with our network.\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name='inputs') # The inputs are stacks of frames.\n",
    "            self.ISWeights_ = tf.placeholder(tf.float32, [None, 1], name='IS_Weights')  # Importance sampling weights.\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name='actions_')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name='target') # targetQ(s, a) = R(s, a) + y*maxQhat(s', a')\n",
    "            \n",
    "            # Creating the actual layers of our network.\n",
    "            \n",
    "            '''\n",
    "            First convolutional layer:\n",
    "            CNN -> ELU activation\n",
    "            '''\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8, 8],\n",
    "                                          strides=[4, 4],\n",
    "                                          padding='VALID',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name='conv1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name='conv1_out')\n",
    "            \n",
    "            '''\n",
    "            Second convolutional layer:\n",
    "            CNN -> ELU activation\n",
    "            '''\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[2, 2],\n",
    "                                          padding='VALID',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name='conv2')\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv1, name='conv2_out')\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Third convolutional layer:\n",
    "            CNN -> ELU activation\n",
    "            '''\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=128,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[2, 2],\n",
    "                                          padding='VALID',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name='conv3')\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv1, name='conv3_out')\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            \n",
    "            # Now we separate our network's stream in two layers - one for V(s) and the other for A(s, a).\n",
    "            \n",
    "            '''\n",
    "            Stream that calculates V(s), a.k.a value layer:\n",
    "            Flatten -> Dense\n",
    "            '''\n",
    "            \n",
    "            self.value_fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                            units=512,\n",
    "                                            activation=tf.nn.elu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name='value_fc')\n",
    "            \n",
    "            self.value = tf.layers.dense(inputs=self.value_fc,\n",
    "                                         units=1,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name='value')\n",
    "            \n",
    "            '''\n",
    "            Stream that calculates A(s, a), a.k.a advantage layer:\n",
    "            Flatten -> Dense\n",
    "            '''\n",
    "\n",
    "            self.advantage_fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                                units=512,\n",
    "                                                activation=tf.nn.elu,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name='advantage_fc')\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs=self.advantage_fc,\n",
    "                                             units=1,\n",
    "                                             activation=None,\n",
    "                                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                             name='advantages')\n",
    "            \n",
    "            \n",
    "            # After creating the advantage and value layers, we create the aggregate to put together all values.\n",
    "            # To do so, it follows the formula: Q(s, a) = V(s) + (A(s, a) - 1 / |A| * sumA(s, a'))\n",
    "            \n",
    "            self.output = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keep_dims=True))\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1) # Q is the prediction of our network\n",
    "            \n",
    "            # Defining loss/optimizer\n",
    "            \n",
    "            # We're modifying the loss because of Prioritized Experience Replay.\n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)  # To update our Sumtree for PER\n",
    "            self.loss = tf.reduce_mean(self.ISWeights_ * tf.squared_difference(self.target_Q, self.Q)) \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-b3e085c3dfbc>:111: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Resetting the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiating our newly created DDQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiating the target network, used to update target values when requested\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Prioritized Experience Replay (PER)\n",
    "\n",
    "The idea behind **PER** is that **some experiences are more important than others for our training** but they might occur less frequently, so instead of sampling experiences through an uniform distribution, we **assign priorities for our experiences in order to improve our sampling**.\n",
    "\n",
    "To implement this technique, we can't use an array, because then sampling would be inefficient. So instead, we'll implement  a **sumtree**, which is a binary tree data type where **parent nodes are the sum of child nodes**.\n",
    "\n",
    "To summarize, our implementation follows these steps:\n",
    "\n",
    "1. First we construct our Sumtree, which is a Binary Tree whose leaves contains the **priorities** and a **data array** with indexed elements that points to the index of leaves.\n",
    "![title](sumtree_explanation.png)\n",
    "    - **def init**: Initializes our SumTree data object with all nodes = 0 and data array with all = 0.\n",
    "    - **def add**: Adds our priority score to the Sumtree's leaf and experience (S, A, R, S', Done) to the data.\n",
    "    - **def update**: We update the leaf's priority score and propagate it through tree.\n",
    "    - **def get_leaf**: Returns the priority score, index and experience associated to a leaf.\n",
    "    - **def total_priority**: Returns the root node value, which is the sum of all child nodes, or the total priority score of our replay buffer.\n",
    "\n",
    "\n",
    "2. Then we create a Memory type object, which will contain our Sumtree and data.\n",
    "    - **def init**: Generates the SumTree and data by instantiating the SumTree object.\n",
    "    - **def store**: Stores a new experience in our SumTree. New experiences are initialized with max priority, and this priority is updated during training, when we calculate our TD error.\n",
    "    - **def sample**:\n",
    "        - To sample a minibatch of k elements, we first divide the range [0, total_priority] into k ranges.\n",
    "        - Then we sample a value uniformly from each range.\n",
    "        - We then search in the SumTree the experience whose priority score corresponds to the sampled values.\n",
    "        - Finally, we calculate IS weights for each element of the minibatch.\n",
    "    - **def update_batch**: Updates the priorities on the tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step of the PER implementation.\n",
    "\n",
    "class Sumtree(object):\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializes the tree with all nodes = 0 and data array with all = 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # Total #leaf_nodes, which are final nodes that contains experience\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adds priority and data to the SumTree. Priority is added to a leaf node, and experience is added to the data array.\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        tree_index = self.data_pointer + self.capacity - 1   # Looking at what index to put the experience in\n",
    "        \n",
    "        self.data[self.data_pointer] = data  # Updating the data frame\n",
    "        self.update(tree_index, priority)  # Updating the leaf node\n",
    "        \n",
    "        self.data_pointer += 1  # Incrementing the data_pointer by 1.\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # Resets the data_pointer if we're above capacity\n",
    "            self.data_pointer = 0\n",
    "        \n",
    "    \"\"\"\n",
    "    Updates the leaf node's priority score, and propagates this change along the SumTree.\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        change = priority - self.tree[tree_index]  # New priority score - former priority score\n",
    "        self.tree[tree_index] = priority  # Update the leaf node's priority score\n",
    "        \n",
    "        # Propagating the change through the tree\n",
    "        while tree_index != 0:\n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the index, priority score and experience associated to a leaf.\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # Ends the search if the bottom of the tree was reached\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:  # The search hasn't ended, so we search downwards for a higher priority node\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "                    \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        \n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "                \n",
    "    \"\"\"\n",
    "    Returns the root node of the tree, which corresponds to the total priority.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step of the PER implementation\n",
    "\n",
    "class Memory(object):\n",
    "    \"\"\"\n",
    "    Defining hyperparameters for our memory\n",
    "    \"\"\"\n",
    "    PER_e = 0.01 # Fixed experience probability, so that experiences don't have 0 prob of being selected.\n",
    "    PER_a = 0.6 # Controls the proportion between sampling only experiences with high priority and sampling randomly\n",
    "    PER_b = 0.4 # Importance sampling's initial value that increases to 1 during training.\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.0  # Clipped absolute error\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates the SumTree and data by instantiating the SumTree object.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # We don't use deque because it requires that indices are changed by 1 every timestep, causing performance issues.\n",
    "        self.tree = Sumtree(capacity)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    Stores a new experience in our SumTree, new experiences are initialized with max priority. This priority is\n",
    "    updated when we use it to train our DDDQNN, which is the moment when we calculate our TD error.\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])  # Finding the max priority for our tree\n",
    "        \n",
    "        # If max_priority is 0, we set it to absolute_error_upper, or the experience would never be selected.\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "            \n",
    "        self.tree.add(max_priority, experience)  # Sets the max priority for the new experience p\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    1) To sample a minibatch of k elements, we first divide the range [0, total_priority] into k ranges.\n",
    "    2) Then we sample a value uniformly from each range.\n",
    "    3) We then search in the SumTree the experience whose priority score corresponds to the sampled values.\n",
    "    4) Finally, we calculate IS weights for each element of the minibatch.\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        memory_batch = []  # This array will contain the minibatch of size n\n",
    "        \n",
    "        b_idx = np.empty((n, ), dtype=np.int32)\n",
    "        b_ISWeights = np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # 1) Dividing the priority range [0, max_priority] into n segments.\n",
    "        priority_segment = self.tree.total_priority / n\n",
    "        \n",
    "        # Increments PER_b each time we sample a new batch. It can be incremented up to 1.\n",
    "        self.PER_b = np.min([1.0, self.PER_b + self.PER_b_increment_per_sampling])\n",
    "        \n",
    "        # Calculating the maximum weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            # 2) Sampling a value uniformly from each range.\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            # 3) Retrieving the experience that corresponds to the sampled values.\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            # prob(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            # 4) Calculating IS weights for the element, which is then added to the minibatch\n",
    "            # IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            \n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b) / max_weight\n",
    "            b_idx[i] = index\n",
    "            experience = [data]\n",
    "            memory_batch.append(experience)\n",
    "        \n",
    "        return b_idx, memory_batch, b_ISWeights\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Updates the priorities on the tree.\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # Convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "        \n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the **empty memory problem**. We'll fill our Memory by taking random actions and storing the experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:51<00:00, 193.90it/s]\n"
     ]
    }
   ],
   "source": [
    "memory = Memory(memory_size) # Instantiating the memory\n",
    "\n",
    "game.new_episode()\n",
    "\n",
    "for i in tqdm(range(pretrain_length)):\n",
    "    # If it's the first step, we initialize the state with a frame and stack it into 4 frames to make our first input.\n",
    "    if i == 0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)  # Samples a random action\n",
    "    reward = game.make_action(action)  # Executes the chosen action and gets the reward\n",
    "    done = game.is_episode_finished()  # Checks if the game has reached a terminal state\n",
    "    \n",
    "    # If we reach a terminal state (win or the character is dead), we reset the environment to restart a new episode\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)  # Setting the next_state to a zero state\n",
    "        \n",
    "        # Adding the current experience to the memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        game.new_episode()  # Starting a new episode\n",
    "        state = game.get_state().screen_buffer  # Restarts the state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  # Creates a new stack of frames\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Adding the current experience to the memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Configurando o TensorBoard\n",
    "\n",
    "O *TensorBoard* é uma ferramenta de análise do *TensorFlow*, assistir https://www.youtube.com/watch?v=eBbEDRsCmv4 .\n",
    "\n",
    "Para executar o *TensorBoard*, devemos utilizar o comando *tensorboard --logdir=/tensorboard/dddqn/1* no CMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/tensorboard/dddqn/1\")  # Setting up TensorBoard's writer\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)  # Recording losses\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Treinando nosso agente\n",
    "\n",
    "O algoritmo que utilizaremos para treinar nosso agente segue os seguintes passos:\n",
    "\n",
    "- **Inicializa** os pesos para a DQN;\n",
    "- **Inicializa** os para a rede que treinará nosso Q-Targets, w- <- w;\n",
    "- **Inicializa** o ambiente Doom;\n",
    "- **Inicializa** a taxa de decaimento usada pela seleção de ações epsilon greedy;\n",
    "\n",
    "- **Para cada** episódio **em** max_episódios:\n",
    "    - **Inicializa** um episódio;\n",
    "    - **Setta** passos = 0\n",
    "    - **Observa** o primeiro estado s_0\n",
    "    \n",
    "    - **Enquanto** passos < max_passos:\n",
    "        - **Incrementa** a taxa de decaimento;\n",
    "        - Com epsilon, **seleciona** um ação aleatória a_t, **caso contrário**, a_t = argmax_a Q(s_t, a), ou seja, **seleciona** a ação com maior Q atualmente;\n",
    "        - **Executa** a ação a_t no simulador e **observa** a recompensa r_t+1 e o novo estado s_t+1;\n",
    "        - **Armazena** a transição $;\n",
    "        - **Amostra** um minibatch aleatório de D, chamado $$;\n",
    "        - **Se** o episódio termina em +1, **setta** target Q_hat = r. **Caso contrário**, **setta** Q_hat = r + y.Q(s', argmax_a'Q(s', a', w), w-);\n",
    "        - **Executa** o gradiente descendente com loss = (Q_hat - Q(s, a))^2;\n",
    "        - A cada tau passos, **resetta** w- = w (passo da estratégia fixed Q-Targets).\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will execute the epsilon greedy action selection for our training.\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    # >>>> EPSILON GREEDY STRATEGY <<<<\n",
    "    # Choosing action a from state s using epsilon greedy\n",
    "    \n",
    "    # First, we pick a random number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    # np.exp calculates the exponential of input parameter.\n",
    "    # Calculating the explore probability. Exploring is the same as picking a random action.\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if(explore_probability > exp_exp_tradeoff):\n",
    "        # We take a random action\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # We get the current best action from our Q-Network. This is exploitation, the opposite of exploration.\n",
    "        \n",
    "        # Estimating the Q-Values for state.\n",
    "        Qs = sess.run(DQNetwork.output,\n",
    "                      feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Getting the biggest Q-Value from our estimated Q-Values. This will yield us the best action.\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function that copies one set of variables to another. This is used to update our fixed Q-Targets network's weights.\n",
    "def update_target_graph():\n",
    "    # Getting the variables from our DQNetwork and target network. We'll copy from DQNetwork to target network\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "    \n",
    "    op_holder = []\n",
    "    \n",
    "    # Updating the target network's parameters using DQNetwork's parameters\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: -91.64329528808594 Training loss: 0.4634 Explore Prob.: 1.0000\n",
      "Model saved!\n",
      "Episode: 1 Total reward: -108.10116577148438 Training loss: 1.0179 Explore Prob.: 1.0000\n",
      "Episode: 2 Total reward: -92.45686340332031 Training loss: 1.0118 Explore Prob.: 1.0000\n",
      "Episode: 3 Total reward: -112.77201843261719 Training loss: 0.6964 Explore Prob.: 1.0000\n",
      "Episode: 4 Total reward: -111.99989318847656 Training loss: 19.2720 Explore Prob.: 1.0000\n",
      "Episode: 5 Total reward: -109.731201171875 Training loss: 14.8977 Explore Prob.: 1.0000\n",
      "Model saved!\n",
      "Episode: 6 Total reward: -115.99786376953125 Training loss: 0.4588 Explore Prob.: 1.0000\n",
      "Episode: 7 Total reward: -111.05291748046875 Training loss: 0.9718 Explore Prob.: 1.0000\n",
      "Episode: 8 Total reward: -95.94459533691406 Training loss: 19.6915 Explore Prob.: 1.0000\n",
      "Episode: 9 Total reward: -106.89639282226562 Training loss: 13.1753 Explore Prob.: 1.0000\n",
      "Episode: 10 Total reward: -92.26089477539062 Training loss: 5.8538 Explore Prob.: 1.0000\n",
      "Model saved!\n",
      "Episode: 11 Total reward: -69.24055480957031 Training loss: 1.0561 Explore Prob.: 1.0000\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-69586bb9bebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m                                                              possible_actions)\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Executing the selected action and observing the reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Checking if the game has reached a terminal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mepisode_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()  # Saves our trained model\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())  # Initializing TensorFlow variables\n",
    "        \n",
    "        decay_step = 0  # Initializing decay rate for epsilon greedy\n",
    "        tau = 0\n",
    "        \n",
    "        game.init()\n",
    "        \n",
    "        # Updating our target network's weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            # >>>> SIMULATION PART <<<<\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            \n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step <  max_steps:\n",
    "                step += 1\n",
    "                tau += 1\n",
    "                decay_rate += 1\n",
    "                \n",
    "                # Selecting a random action using epsilon greedy\n",
    "                action, explore_probability = predict_action(explore_start,\n",
    "                                                             explore_stop,\n",
    "                                                             decay_rate,\n",
    "                                                             decay_step,\n",
    "                                                             state,\n",
    "                                                             possible_actions)\n",
    "                \n",
    "                reward = game.make_action(action)  # Executing the selected action and observing the reward\n",
    "                done = game.is_episode_finished()  # Checking if the game has reached a terminal state\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # If the agent reached a terminal state, we set next_state to an empty state, observe the reward\n",
    "                # and store the experience tuple in Memory.\n",
    "                if done:\n",
    "                    next_state = np.zeros((3, 240, 320), dtype=np.int)  # Setting next_state to empty\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    step = max_steps # Setting step to max steps so that the episode is ended\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print(\"Episode: {}\".format(episode),\n",
    "                          \"Total reward: {}\".format(total_reward),\n",
    "                          \"Training loss: {:.4f}\".format(loss),\n",
    "                          \"Explore Prob.: {:.4f}\".format(explore_probability))\n",
    "                    \n",
    "                    # Storing the experience in memory.\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                else: # If the agent hasn't reached a terminal state, we get the next state and store the experience.\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    # Storing the experience in memory.\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    state = next_state\n",
    "                \n",
    "                # >>>> LEARNING PART <<<<\n",
    "                \n",
    "                # Getting a random minibatch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                states_mb = np.array([exp_tuple[0][0] for exp_tuple in batch], ndmin=3)\n",
    "                actions_mb = np.array([exp_tuple[0][1] for exp_tuple in batch])\n",
    "                rewards_mb = np.array([exp_tuple[0][2] for exp_tuple in batch])\n",
    "                next_states_mb = np.array([exp_tuple[0][3] for exp_tuple in batch], ndmin=3)\n",
    "                dones_mb = np.array([exp_tuple[0][4] for exp_tuple in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                # DOUBLE DQN LOGIC\n",
    "                # In Double DQN we use the DQNNetwork to select the a' action to take at the next state s',\n",
    "                # the one with the highest Q-Value.\n",
    "                # Then we use TargetNetwork to calculate the Q_Val of Q(s', a')\n",
    "                \n",
    "                \n",
    "                # Getting Q-Values for next state\n",
    "                q_next_state = sess.run(DQNetwork.output,\n",
    "                                        feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Calculating the Q-Target for all actions at that state\n",
    "                q_target_next_state = sess.run(TargetNetwork.output,\n",
    "                                               feed_dict = {TargetNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Sets QTarget = r if the state ends at +1, otherwise, sets Q_target = r + gamma * Qtarget(s',a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    action = np.argmax(q_next_state[i])  # Getting the best action from next state, a'\n",
    "                    \n",
    "                    # If the state is terminal, QTarget = r\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    # If the state is not terminal, we set QTarget = r + gamma * QTarget(s', a')\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([exp_tuple for exp_tuple in target_Qs_batch])\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer, DQNetwork.loss, DQNetwork.absolute_errors],\n",
    "                                                    feed_dict = {\n",
    "                                                        DQNetwork.inputs_: states_mb,\n",
    "                                                        DQNetwork.target_Q: targets_mb,\n",
    "                                                        DQNetwork.actions_: actions_mb,\n",
    "                                                        DQNetwork.ISWeights_: ISWeights_mb\n",
    "                                                    })\n",
    "                \n",
    "                # Updating the priorities for PER experiences.\n",
    "                memory.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "                # Writing tf summaries to TensorBoard\n",
    "                summary = sess.run(write_op,\n",
    "                                   feed_dict = {\n",
    "                                        DQNetwork.inputs_: states_mb,\n",
    "                                        DQNetwork.target_Q: targets_mb,\n",
    "                                        DQNetwork.actions_: actions_mb,\n",
    "                                        DQNetwork.ISWeights_: ISWeights_mb\n",
    "                                   })\n",
    "                \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                # If tau > max_tau, we update the weights of our target network\n",
    "                if tau > max_tau:\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Target network weights updated!\")\n",
    "            \n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"models/corridor_model.ckpt\")\n",
    "                print(\"Model saved!\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Assistindo nosso agente jogar.\n",
    "\n",
    "Agora que treinamos nosso agente, podemos assistí-lo jogar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/corridor_model.ckpt\n",
      "Score:  -115.81562805175781\n",
      "Score:  -106.03758239746094\n",
      "Score:  -115.81562805175781\n",
      "Score:  -115.81562805175781\n",
      "Score:  -115.9755859375\n",
      "Score:  -115.81562805175781\n",
      "Score:  -115.81562805175781\n",
      "Score:  -115.98381042480469\n",
      "Score:  -115.99192810058594\n",
      "Score:  -115.81562805175781\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    game = DoomGame()\n",
    "    \n",
    "    game.load_config(config_filename)\n",
    "    game.set_doom_scenario_path(scenario_filename)\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    saver.restore(sess, \"models/corridor_model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            # Choose an action using epsilon greedy\n",
    "            \n",
    "            exp_exp_tradeoff = np.random.rand() # Exploration/exploration tradeoff ratio\n",
    "            \n",
    "            explore_probability = 0.01\n",
    "            \n",
    "            if explore_probability > exp_exp_tradeoff:\n",
    "                action = random.choice(possible_actions)  # Getting a random action\n",
    "                \n",
    "            else:\n",
    "                # Getting predicted Q-Values from the network\n",
    "                Qs = sess.run(DQNetwork.output,\n",
    "                              feed_dict= {\n",
    "                                  DQNetwork.inputs_: state.reshape((1, *state.shape))\n",
    "                              })\n",
    "                choice = np.argmax(Qs) # Picks the greatest Q-Value from the network\n",
    "                action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "        \n",
    "    game.close()\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
