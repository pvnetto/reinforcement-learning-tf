{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Installing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (0.32.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.15.4)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.11.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.0.5)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (3.6.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorflow-gpu) (1.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu) (40.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu) (0.14.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (1.15.4)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (5.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (1.11.0)\n",
      "Requirement already satisfied: dask[array]>=0.9.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (0.20.0)\n",
      "Requirement already satisfied: networkx>=1.8 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (0.6.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from scikit-image) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.0; extra == \"array\" in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from dask[array]>=0.9.0->scikit-image) (1.15.4)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from dask[array]>=0.9.0->scikit-image) (0.9.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from networkx>=1.8->scikit-image) (4.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from matplotlib) (1.15.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from matplotlib) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied: six in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\usuario\\anaconda3\\envs\\tfp36\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (40.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Vizdoom installation guide and download link can be found at: https://github.com/mwydmuch/ViZDoom\n",
    "!pip install tensorflow-gpu\n",
    "!pip install numpy\n",
    "!pip install scikit-image\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "from vizdoom import *\n",
    "\n",
    "# Ignores messages that are printed during training by skimage.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Creating our Doom environment\n",
    "\n",
    "### Our scenario\n",
    "\n",
    "This time, the scenario we'll use in our environment consists of a stage with multiple MedKits spread around it and the agent constantly losing health. Through the rewards, the agent only knows that life is good and death is bad, so we need to **train our agent to pick the MedKits around the stage so he can survive**. MedKits also spawn periodically and are uniformly distributed around the stage.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "To create our Doom environment we're using the [VizDoom library](https://github.com/mwydmuch/ViZDoom), which in our case will be configured as follows.\n",
    "- Our VizDoom environment takes:\n",
    "    - A *configuration file* that handles all the options, like size of the frame, possible actions etc.\n",
    "    - A *scenario file* that generates the scenario we picked.\n",
    "- We have 3 possible **actions**, namely:\n",
    "    - Move left, encoded as [1, 0, 0]\n",
    "    - Move right, encoded as [0. 0. 1]\n",
    "    - Move forward, encoded as [0, 1, 0]\n",
    "- The **reward** for living is **1**.\n",
    "- The **reward** for dying is **-100**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating our Doom environment with VizDoom.\n",
    "'''\n",
    "\n",
    "def create_env():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    game.load_config('health_gathering.cfg')   # Loading the configuration file\n",
    "    \n",
    "    game.set_doom_scenario_path('health_gathering.wad')   # Loading our scenario\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # The possible actions are:\n",
    "    # move_left: [1, 0, 0]\n",
    "    # move_forward: [0, 1, 0]\n",
    "    # move_right: [0, 0, 1]\n",
    "    possible_actions = np.identity(3, dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Defining the preprocess function\n",
    "\n",
    "In order to **reduce the size of our states and the complexity of processing them, which consequently reduces the time required for training**, we'll preprocess our frames with a function that follows these steps:\n",
    "\n",
    "- **Grayscale** each frame, since **color doesn't add important information** and the frames we get from VizDoom come with RGB color channels.\n",
    "- **Crop the screen** to remove the roof, since it also doesn't add important information.\n",
    "- **Normalize** pixel values. We should always use normalized data to train our networks.\n",
    "- **Resize** the preprocessed frames to a squared size (nxn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Grayscales, crops, normalizes and resizes the input frame.\n",
    "'''\n",
    "def preprocess_frame(frame):\n",
    "    # Grayscaling our frame\n",
    "    grayscaled_frame = np.mean(frame, axis=0)\n",
    "    \n",
    "    # Cropping our frame to remove the roof\n",
    "    cropped_frame = grayscaled_frame[80:,:]\n",
    "    \n",
    "    # Normalizing pixel values\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "    \n",
    "    # Resizing the frame to (84, 84)\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking frames\n",
    "\n",
    "We need to stack frames in order to **give our network a sense of motion**. To do so, we'll follow the following steps:\n",
    "\n",
    "- Preprocess the current frame.\n",
    "- Push the preprocessed frame to a *deque*, which automatically removes the oldest frame.\n",
    "- Then, we build our stacked state, which consists of a state represented by a stack of frames.\n",
    "\n",
    "The stacking works as follows:\n",
    "\n",
    "- For the first frame/state, we'll repeat it n times, n being the maximum size of our stack.\n",
    "- For each timestep of our training, we'll add a new frame to the deque, so we can stack them in order to get our stacked state.\n",
    "- We follow these steps until the end of the episode.\n",
    "- For each episode, we repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4  # The number of frames we'll stack together in our stacked state\n",
    "\n",
    "# Initializing our stacked state with 'stack_size' empty states\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    \n",
    "    # Preprocessing the input frame (state)\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Since we're in a new episode, we fill our stack of frames using the same frame\n",
    "        stacked_frames = deque([frame for i in range(stack_size)], maxlen=stack_size)\n",
    "        \n",
    "    else:\n",
    "        # We append the input frame to our deque, which automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "    \n",
    "    # Stacking the frames on stacked_frames to build our stacked_state\n",
    "    stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounting and normalizing rewards\n",
    "\n",
    "Since we're in a **Monte Carlo situation** we need to **discount the rewards at the end of the episode**. To do so, we'll implement a funtion that:\n",
    "\n",
    "1. Takes a list with all the rewards for an episode.\n",
    "2. Applies our previously defined gamma to discount them.\n",
    "3. Normalizes the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Takes as input a list with all the rewards for an episode\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    \n",
    "    # 2) Calculating the discounted rewards\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    # 3) Normalizing the rewards by subtracting them to their mean and dividing by the standard deviation\n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Setting up our hyperparameters\n",
    "\n",
    "In this guide, we'll set all our hyperparameters at once, that is, the model and training hyperparameters. But in a regular reinforcement learning workflow, we would do it progressively:\n",
    "\n",
    "- First, we would begin by defining the NN hyperparameters when we implement the model.\n",
    "- Then, we'd add the training parameters when implementing the training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### >>>> ENVIRONMENT PARAMETERS <<<<\n",
    "state_size = [84, 84, stack_size]  # We'll have a stack of 'stack_size' frames with (84, 84) size\n",
    "action_size = game.get_available_buttons_size()  # The default #actions for this scenario is 3\n",
    "\n",
    "# >>>> TRAINING PARAMETERS <<<<\n",
    "learning_rate = 0.002\n",
    "num_epochs = 500\n",
    "\n",
    "batch_size = 5000  # The number of timesteps (1000 if cpu, 5000 if gpu)\n",
    "gamma = 0.95  # Reward discount rate\n",
    "\n",
    "# This should be set to False if only want to watch the agent without training\n",
    "training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Creating our Policy Gradient Neural Network model\n",
    "\n",
    "The model we'll implement has the following configuration:\n",
    "\n",
    "- Input layer which receives a stack of 'state_size' preprocessed stacked frames\n",
    "- Three Convolutional layers with ELU activation and batch normalization\n",
    "- Flatten layer\n",
    "- Dense layer with 512 units and ELU activation\n",
    "- Dense layer with 'action_size' units and no activation (logits).\n",
    "- Output: Softmax activation over logits, which returns a probability distribution over all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope('inputs'):\n",
    "                \n",
    "                # Defining placeholders for inputs, actions discounted rewards and rewards.\n",
    "                \n",
    "                # Our network will receive a stack of preprocessed frames when we initialize it.\n",
    "                self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name='inputs_')\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name='actions')\n",
    "                self.discounted_episode_rewards = tf.placeholder(tf.float32, [None, ], name='discounted_episode_rewards_')\n",
    "                \n",
    "                # This placeholder is used to write the mean reward values to TensorBoard\n",
    "                self.mean_reward_ = tf.placeholder(tf.float32, name='mean_reward')\n",
    "                \n",
    "                with tf.name_scope('conv1'):\n",
    "                    \n",
    "                    '''\n",
    "                    First convolutional layer:\n",
    "                    CNN -> Batch Normalization -> ELU\n",
    "                    '''\n",
    "                    \n",
    "                    self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                                  filters=32,\n",
    "                                                  kernel_size=[8, 8],\n",
    "                                                  strides=[4,4],\n",
    "                                                  padding='VALID',\n",
    "                                                  kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                                  name='conv1')\n",
    "                    \n",
    "                    self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                                         training=True,\n",
    "                                                                         epsilon=1e-5,\n",
    "                                                                         name='batch_norm1')\n",
    "                    \n",
    "                    self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name='conv1_out')\n",
    "                    \n",
    "                with tf.name_scope('conv2'):\n",
    "                    '''\n",
    "                    Second convolutional layer:\n",
    "                    CNN -> Batch Normalization -> ELU\n",
    "                    '''\n",
    "                    \n",
    "                    self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                                  filters=64,\n",
    "                                                  kernel_size=[4, 4],\n",
    "                                                  strides=[2, 2],\n",
    "                                                  padding='VALID',\n",
    "                                                  kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                                  name='conv2')\n",
    "                    \n",
    "                    self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                                         training=True,\n",
    "                                                                         epsilon=1e-5,\n",
    "                                                                         name='batch_norm2')\n",
    "                    \n",
    "                    self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name='conv2_out')\n",
    "                    \n",
    "                with tf.name_scope('conv3'):\n",
    "                    '''\n",
    "                    Third convolutional layer:\n",
    "                    CNN -> Batch Normalization -> ELU\n",
    "                    '''\n",
    "                    \n",
    "                    self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                                  filters=128,\n",
    "                                                  kernel_size=[4, 4],\n",
    "                                                  strides=[2, 2],\n",
    "                                                  padding='VALID',\n",
    "                                                  kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                                  name='conv3')\n",
    "                    \n",
    "                    self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                                         training=True,\n",
    "                                                                         epsilon=1e-5,\n",
    "                                                                         name='batch_norm3')\n",
    "                    \n",
    "                    self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name='conv3_out')\n",
    "                    \n",
    "                with tf.name_scope('flatten'):\n",
    "                    self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "                    \n",
    "                with tf.name_scope('fc1'):\n",
    "                    self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                              units=512,\n",
    "                                              activation=tf.nn.elu,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              name='fc1')\n",
    "                    \n",
    "                with tf.name_scope('logits'):\n",
    "                    self.logits = tf.layers.dense(inputs=self.fc,\n",
    "                                                  kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                  units=self.action_size,\n",
    "                                                  activation=None)\n",
    "                    \n",
    "                with tf.name_scope('softmax'):\n",
    "                    self.action_distribution = tf.nn.softmax(self.logits)\n",
    "                    \n",
    "                with tf.name_scope('loss'):\n",
    "                    # Since we're dealing with a network that classifies our actions into multiple\n",
    "                    # classes by using a probability distribution, we'll use softmax crossentropy\n",
    "                    # to compute our loss. If we were classifying with 1 class, we would use \n",
    "                    # tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "                    self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.actions)\n",
    "                    self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards)\n",
    "                    \n",
    "                with tf.name_scope('train'):\n",
    "                    self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiating our Policy Gradient Neural Network\n",
    "PGNetwork = PGNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "# Initializing our TensorFlow session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Setting up TensorBoard\n",
    "\n",
    "Before running this cell, make sure to run *tensorboard --logdir=/tensorboard/pg/1* on the command prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up TensorBoard writer\n",
    "writer = tf.summary.FileWriter('/tensorboard/pg/1')\n",
    "\n",
    "# Subscribes the loss to TensorBoard\n",
    "tf.summary.scalar('Loss', PGNetwork.loss)\n",
    "\n",
    "# Subscribes the reward mean to TensorBoard\n",
    "tf.summary.scalar('Reward_mean', PGNetwork.mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Training our agent.\n",
    "\n",
    "Now we'll create batches. These batches contains **episodes**, and the number of episodes for each batch depends on the number of rewards we collect. Ex: If we have **episodes** with **only 10 rewards**, we can make **batch_size / 10** episodes. So our function will do the following:\n",
    "\n",
    "- Make a batch:\n",
    "    - For each **step**:\n",
    "        - **Choose** *action* *a*\n",
    "        - **Perform** *action* *a*\n",
    "        - **Store** *s*, *a*, *r*\n",
    "        - **If** *done*:\n",
    "            - **Calculate** the sum of all rewards\n",
    "            - **Calculate** *gamma* Gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size, stacked_frames):\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\n",
    "    \n",
    "    # Reward of batch is also used to keep track of how many timesteps we made. It's used to verify\n",
    "    # at the end of each episode if > batch_size.\n",
    "    episode_num = 1  # Keeps track of the number of episodes in our batch\n",
    "    \n",
    "    game.new_episode()\n",
    "    \n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    while True:\n",
    "        # Runs the neural network with our current stack of frames to get the action probability distribution\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution,\n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "        \n",
    "        # Choosing an action using the probability distribution from our network\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]),\n",
    "                                  p=action_probability_distribution.ravel())\n",
    "        action = possible_actions[action]\n",
    "        \n",
    "        # Performs the chosen action\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "    \n",
    "        # Storing the results\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards_of_episode.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # The episode is over, so we set next_state to empty\n",
    "            next_state = np.zeros((3, 240, 240), dtype=np.int)\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            \n",
    "            # Appending the rewards_of_episode to rewards_of_batch\n",
    "            rewards_of_batch.append(rewards_of_episode)\n",
    "            \n",
    "            # Calculating gamma Gt\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\n",
    "            \n",
    "            # len(rewards_of_batch) = num episodes, and if it's bigger than batch_size, we should\n",
    "            # stop the minibatch creation. This condition is only checked when an episode is finished.\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Resetting the rewards_of_episode, since the episode is finished.\n",
    "            rewards_of_episode = []\n",
    "            \n",
    "            episode_num += 1\n",
    "            game.new_episode()\n",
    "            \n",
    "            # Resetting the stack of frames before starting a new episode\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        else:\n",
    "            # If the episode is not over, next_state = current_state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "    \n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Neural Network\n",
    "\n",
    "- Initialize the weights\n",
    "- Initialize the Doom environment\n",
    "- Set max_reward = 0 to keep track of the maximum reward\n",
    "- For each epoch in range(num_epochs):\n",
    "    - Get batches\n",
    "    - Optimize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Epoch:  1 / 500\n",
      "----------\n",
      "Number of training episodes: 12\n",
      "Total reward: 3984.0\n",
      "Mean reward of current batch: 332.0\n",
      "Average reward of all training: 332.0\n",
      "Max reward of all batches so far: 3984.0\n",
      "Training Loss: -0.01314134057611227\n",
      "====================================\n",
      "Epoch:  2 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4052.0\n",
      "Mean reward of current batch: 368.3636363636364\n",
      "Average reward of all training: 350.1818181818182\n",
      "Max reward of all batches so far: 4052.0\n",
      "Training Loss: 0.007343720179051161\n",
      "====================================\n",
      "Epoch:  3 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4052.0\n",
      "Mean reward of current batch: 368.3636363636364\n",
      "Average reward of all training: 356.24242424242425\n",
      "Max reward of all batches so far: 4052.0\n",
      "Training Loss: -0.0011698236921802163\n",
      "====================================\n",
      "Epoch:  4 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4180.0\n",
      "Mean reward of current batch: 380.0\n",
      "Average reward of all training: 362.1818181818182\n",
      "Max reward of all batches so far: 4180.0\n",
      "Training Loss: 0.012000523507595062\n",
      "====================================\n",
      "Epoch:  5 / 500\n",
      "----------\n",
      "Number of training episodes: 12\n",
      "Total reward: 4272.0\n",
      "Mean reward of current batch: 356.0\n",
      "Average reward of all training: 360.94545454545454\n",
      "Max reward of all batches so far: 4272.0\n",
      "Training Loss: -0.0019950629211962223\n",
      "====================================\n",
      "Epoch:  6 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4244.0\n",
      "Mean reward of current batch: 385.8181818181818\n",
      "Average reward of all training: 365.09090909090907\n",
      "Max reward of all batches so far: 4272.0\n",
      "Training Loss: -0.0014114758232608438\n",
      "====================================\n",
      "Epoch:  7 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4308.0\n",
      "Mean reward of current batch: 391.6363636363636\n",
      "Average reward of all training: 368.8831168831169\n",
      "Max reward of all batches so far: 4308.0\n",
      "Training Loss: -0.0026065486017614603\n",
      "====================================\n",
      "Epoch:  8 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4180.0\n",
      "Mean reward of current batch: 380.0\n",
      "Average reward of all training: 370.27272727272725\n",
      "Max reward of all batches so far: 4308.0\n",
      "Training Loss: -0.009214624762535095\n",
      "====================================\n",
      "Epoch:  9 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 3924.0\n",
      "Mean reward of current batch: 356.72727272727275\n",
      "Average reward of all training: 368.7676767676768\n",
      "Max reward of all batches so far: 4308.0\n",
      "Training Loss: 0.001168800750747323\n",
      "====================================\n",
      "Epoch:  10 / 500\n",
      "----------\n",
      "Number of training episodes: 11\n",
      "Total reward: 4276.0\n",
      "Mean reward of current batch: 388.72727272727275\n",
      "Average reward of all training: 370.7636363636364\n",
      "Max reward of all batches so far: 4308.0\n",
      "Training Loss: -0.00614228006452322\n",
      "Model saved!\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-17dbfcb3d2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Gathers training data from episodes ran in minibatches. mb = minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mstates_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards_of_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscounted_rewards_mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacked_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Calculating values for analytics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-4de1ee70c704>\u001b[0m in \u001b[0;36mmake_batch\u001b[1;34m(batch_size, stacked_frames)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Performs the chosen action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "all_rewards = []  # Keeps track of the total reward for each batch\n",
    "\n",
    "total_rewards = 0\n",
    "maximum_reward_recorded = 0\n",
    "mean_reward_total = []\n",
    "epoch = 1\n",
    "average_reward = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training:\n",
    "    #saver.restore(sess, './models/model.ckpt')  # Loads a previously trained model\n",
    "    \n",
    "    while epoch < num_epochs + 1:\n",
    "        # Gathers training data from episodes ran in minibatches. mb = minibatch\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, num_episodes_mb = make_batch(batch_size, stacked_frames)\n",
    "        \n",
    "        # Calculating values for analytics\n",
    "        total_reward_current_batch = np.sum(rewards_of_batch)\n",
    "        all_rewards.append(total_reward_current_batch)\n",
    "        \n",
    "        mean_reward_current_batch = np.divide(total_reward_current_batch, num_episodes_mb)\n",
    "        mean_reward_total.append(mean_reward_current_batch)\n",
    "        \n",
    "        # Calculates the average reward of the entire training: mean_reward_current_batch / epoch\n",
    "        average_reward_all_training = np.divide(np.sum(mean_reward_total), epoch)\n",
    "        \n",
    "        maximum_reward_recorded = np.amax(all_rewards)\n",
    "        \n",
    "        print('====================================')\n",
    "        print('Epoch: ', epoch, '/', num_epochs)\n",
    "        print('----------')\n",
    "        print('Number of training episodes: {}'.format(num_episodes_mb))\n",
    "        print('Total reward: {}'.format(total_reward_current_batch, num_episodes_mb))\n",
    "        print('Mean reward of current batch: {}'.format(mean_reward_current_batch))\n",
    "        print('Average reward of all training: {}'.format(average_reward_all_training))\n",
    "        print('Max reward of all batches so far: {}'.format(maximum_reward_recorded))\n",
    "        \n",
    "        # Network optimization through Feedforward, gradient and backpropagation\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt],\n",
    "                            feed_dict={\n",
    "                                PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84, 84, 4)),\n",
    "                                PGNetwork.actions: actions_mb,\n",
    "                                PGNetwork.discounted_episode_rewards: discounted_rewards_mb})\n",
    "        \n",
    "        print('Training Loss: {}'.format(loss_))\n",
    "        \n",
    "        # Writing summaries to TensorBoard\n",
    "        summary = sess.run(write_op,\n",
    "                           feed_dict={\n",
    "                               PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84, 84, 4)),\n",
    "                               PGNetwork.actions: actions_mb,\n",
    "                               PGNetwork.discounted_episode_rewards: discounted_rewards_mb,\n",
    "                               PGNetwork.mean_reward_: mean_reward_current_batch\n",
    "                           })\n",
    "        \n",
    "        writer.add_summary(summary)\n",
    "        writer.flush()\n",
    "        \n",
    "        # Saving the trained model\n",
    "        if epoch % 10 == 0:\n",
    "            saver.save(sess, 'models/model.ckpt')\n",
    "            print('Model saved!')\n",
    "        epoch += 1\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Watching our Agent play\n",
    "\n",
    "Now that we trained our agent and saved the model, we can watch him play by doing what he learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
      "Score for episode # 0 :  284.0\n",
      "Score for episode # 1 :  476.0\n",
      "Score for episode # 2 :  284.0\n",
      "Score for episode # 3 :  348.0\n",
      "Score for episode # 4 :  444.0\n",
      "Score for episode # 5 :  476.0\n",
      "Score for episode # 6 :  380.0\n",
      "Score for episode # 7 :  348.0\n",
      "Score for episode # 8 :  348.0\n",
      "Score for episode # 9 :  380.0\n"
     ]
    }
   ],
   "source": [
    "num_matches = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    game = DoomGame()\n",
    "    game.load_config('health_gathering.cfg')\n",
    "    game.set_doom_scenario_path('health_gathering.wad')\n",
    "    \n",
    "    # Loading the model\n",
    "    saver.restore(sess, 'models/model.ckpt')\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(num_matches):\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            action_probability_distribution = sess.run(PGNetwork.action_distribution,\n",
    "                                                       feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\n",
    "            \n",
    "            # Choosing an action from the network's probability distribution over actions\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]),\n",
    "                                      p=action_probability_distribution.ravel())\n",
    "            action = possible_actions[action]\n",
    "            \n",
    "            # Performing the chosen action\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        \n",
    "        print('Score for episode #', i, ': ', game.get_total_reward())\n",
    "        \n",
    "    game.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
